# Common config file for optical cache

[general]
enable_icache_modeling = true

[perf_model/core]
logical_cpus = 1 # number of SMT threads per core
core_model = nehalem
frequency = 5 #3.2
type = rob

[perf_model/core/interval_timer]
dispatch_width = 4 # 4-way issue
window_size = 224 # ROB size
num_outstanding_loadstores = 72

[perf_model/core/rob_timer]
outstanding_loads = 72
outstanding_stores = 56
rs_entries = 224
issue_memops_at_issue = true    # Issue memops to the memory hierarchy at issue time (false = before dispatch)
in_order = false
issue_contention = true
mlp_histogram = false           # Collect histogram of memory-level parallelism (slow)
store_to_load_forwarding = true # Forward data to loads from stores that are still in the store buffer
address_disambiguation = true   # Allow loads to bypass preceding stores with an unknown address
rob_repartition = true          # For SMT model with static ROB partitioning, whether to repartition the ROB
                                # across all active threads (true), or keep everyone fixed at a 1/nthreads share (false)
simultaneous_issue = true       # Whether two different threads can execute in a single cycle. true = simultaneous multi-threading, false = fine-grained multi-threading
commit_width = 4                # Commit bandwidth (instructions per cycle), per SMT thread

[perf_model/sync]
reschedule_cost = 1000

[caching_protocol]
type = parametric_dram_directory_msi
variant = mesi

[perf_model/branch_predictor]
type = pentium_m
mispredict_penalty=26 # 16.5 Reflects just the front-end portion (approx) of the penalty for Interval Simulation, https://www.7-cpu.com/cpu/Skylake.html
size = 4096

[perf_model/tlb]
penalty = 14          # 9 Page walk penalty in cycles

[perf_model/itlb]
size = 128            # Number of I-TLB entries
associativity = 8     # I-TLB associativity

[perf_model/dtlb]
size = 64             # Number of D-TLB entries
associativity = 4     # D-TLB associativity

[perf_model/stlb]
size = 1536            # Number of second-level TLB entries
associativity = 12     # S-TLB associativity

[perf_model/cache]
levels = 2

[perf_model/l1_icache]
perfect = false
cache_size = 1024
associativity = 1
address_hash = mask
replacement_policy = lru
data_access_time = 2
tags_access_time = 2
pcm_write_time = 35
next_level_pcm_write_time = 750
perf_model_type = pcm
writethrough = 0
shared_cores = 16
cache_block_size = 64
outstanding_misses = 10
has_wq = true
wq_size = 10
passthrough = false
writeback_time = 750
wq_access_time = 1

[perf_model/l1_dcache]
perfect = false
cache_size = 4096
associativity = 1
address_hash = mask
replacement_policy = lru
data_access_time = 2
tags_access_time = 2
pcm_write_time = 35
perf_model_type = pcm
next_level_pcm_write_time = 750
writethrough = 0
shared_cores = 16
cache_block_size = 64
bank = 4
outstanding_misses = 10
has_wq = true
wq_size = 10
passthrough = false
writeback_time = 750
wq_access_time = 1

[perf_model/l2_cache]
cache_block_size = 64
address_hash = mask #mask, "mod" so sniper does not report "Caches of non-power of 2 size need funky hash function" error
dvfs_domain = global # L1 and L2 run at core frequency (default), L3 is system frequency
prefetcher = none
writeback_time = 0
perfect = false
cache_size = 32 #
associativity = 1 #11
replacement_policy = lru
data_access_time = 0 #50 #44 # 35 cycles total according to membench, +L1+L2 tag times
tags_access_time = 0 #12.5 #11
pcm_write_time = 0
next_level_pcm_write_time = 0
perf_model_type = parallel
writethrough = 0
shared_cores = 16
outstanding_misses = 40
has_wq = false
wq_size = 0
passthrough = true
wq_access_time = 0

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map
locations = llc

[perf_model/dram]
type = constant
num_controllers = -1
controllers_interleaving = 16
# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time (14 cycles = 5.2 ns),
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Membench says 175 cycles @ 2.66 GHz = 66 ns total
latency = 49.37 # https://www.anandtech.com/show/11544/intel-skylake-ep-vs-amd-epyc-7000-cpu-battle-of-the-decade/13
per_controller_bandwidth = 21.33         # In GB/s, as measured by core_validation-dram
chips_per_dimm = 8
dimms_per_controller = 6

[perf_model/dram/queue_model]
enabled = true
type = windowed_mg1

[perf_model/nuca]
enabled = true
cache_size = 32768 #26624       # In KB
associativity = 1
address_hash = mask
replacement_policy = lru
tags_access_time = 1    # In cycles
data_access_time = 1    # In cycles, parallel with tag access
pcm_write_time = 750
bandwidth = 28762          # In GB/s
replacement_policy = lru
has_wq = false
wq_size = 0
wq_access_time = 0

[perf_model/nuca/queue_model]
enabled = true
type = windowed_mg1

[network]
collect_traffic_matrix = "true"
memory_model_1 = "bus"
system_model = "magic"

#[network/emesh_hop_by_hop]
#hop_latency = 1            # Per-hop latency in core cycles
#link_bandwidth = 256       # Per-link, per-direction bandwidth in bits/cycle
#dimensions = 2             # Mesh dimensions (1 for line/ring, 2 for mesh/torus)
#wrap_around = false        # Has wrap-around links? (false for line/mesh, true for ring/torus)
#concentration = 1          # Number of cores per network interface (must be >= last-level-cache/shared_cores)
#size = 4:4

#[network/emesh_hop_by_hop/queue_model]
#enabled = true
#type = windowed_mg1

#[network/emesh_hop_by_hop/broadcast_tree]
#enabled = false

[clock_skew_minimization]
scheme = barrier

[clock_skew_minimization/barrier]
quantum = 100

[dvfs]
transition_latency = 2000 # In ns, "under 2 microseconds" according to http://download.intel.com/design/intarch/papers/323671.pdf (page 8)

[dvfs/simple]
cores_per_socket = 1

[power]
vdd = 1.2 # Volts
technology_node = 22 # nm

# [network]
# memory_model_1 = bus
#memory_model_2 = bus

[network/bus]
bandwidth = 1280 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links
